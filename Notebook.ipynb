{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f1b43e6",
   "metadata": {},
   "source": [
    "instalamos librerÃ­as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ead1ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (from requests) (2025.10.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (4.67.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence_transformers in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (5.1.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (from sentence_transformers) (4.56.2)\n",
      "Requirement already satisfied: tqdm in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (from sentence_transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (from sentence_transformers) (2.8.0+cpu)\n",
      "Requirement already satisfied: scikit-learn in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (from sentence_transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (from sentence_transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (from sentence_transformers) (0.35.3)\n",
      "Requirement already satisfied: Pillow in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (from sentence_transformers) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (from sentence_transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.3.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (1.1.10)\n",
      "Requirement already satisfied: setuptools in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (72.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (2025.10.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting toon\n",
      "  Downloading toon-0.15.9.zip (320 kB)\n",
      "  Idone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.1 in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (from toon) (2.1.3)\n",
      "Requirement already satisfied: psutil in /home/moises-portillo/Downloads/Or/lib/python3.13/site-packages (from toon) (5.9.0)\n",
      "Building wheels for collected packages: toon\n",
      "  Building wheel for toon (pyproject.toml) ...done\n",
      "\u001b[?25h  Created wheel for toon: filename=toon-0.15.9-cp313-cp313-linux_x86_64.whl size=204342 sha256=93762bedc8ee4835138fa29138fd62fcc8c03d7c08b447ddbbbb72b5690629af\n",
      "  Stored in directory: /home/moises-portillo/.cache/pip/wheels/59/b1/da/787847fc13ce738a5765534bc8c6696b20fe52e3431a498527\n",
      "Successfully built toon\n",
      "Installing collected packages: toon\n",
      "Successfully installed toon-0.15.9\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install requests\n",
    "! pip install pandas\n",
    "! pip install tqdm\n",
    "! pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fbd9c7",
   "metadata": {},
   "source": [
    "importamos librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60e9243c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17830055",
   "metadata": {},
   "source": [
    "verificamos que estÃ¡ corriendo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bf80d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama estÃ¡ corriendo y accesible\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = requests.get('http://localhost:11434')\n",
    "    if response.status_code == 200 and 'Ollama is running' in response.text:\n",
    "        print(\"Ollama estÃ¡ corriendo y accesible\")\n",
    "    else:\n",
    "        print(f\"Ollama respondiÃ³ con estado {response.status_code}.\")\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"Error de ConexiÃ³n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd8d346",
   "metadata": {},
   "source": [
    "Definimos valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "462a1824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos los valores para el modo RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "# Archivos de entrada/salida\n",
    "ARCHIVO_JSONL = \"train.jsonl\"             # Archivo de preguntas de PRUEBA\n",
    "ARCHIVO_CORPUS = \"corpus.json\"          # ðŸš¨ NUEVO: Nombre del archivo de pasajes de CONTEXTO RAG\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "# ConfiguraciÃ³n de la variante\n",
    "MODEL = \"llama3.1:8b\"\n",
    "#MODEL = \"llama3-13b\"\n",
    "VARIANTE = \"rag\"                         # ðŸš¨ CAMBIO CLAVE: Cambiamos a 'rag'\n",
    "ARCHIVO_CSV_SALIDA = VARIANTE + \".csv\"\n",
    "\n",
    "# Prompt base sin contexto ni Few-Shot (se usa si no hay datos de contexto)\n",
    "PROMPT_BASE = \"\"\"You are an AI model highly specialized in factual information retrieval. You are operating within the temporal context of 2018. All answers provided must strictly reflect the knowledge, events, and state of affairs valid up to the end of that year.\n",
    "\n",
    "Your sole mission is to address the user's question with the absolute minimum number of words possible, delivering only the essential and requested information. If ambiguity arises, assume the user's intended question and prioritize the most probable correct answer without acknowledging the error.\n",
    "\n",
    "The strict instructions that you may follow are the next ones:\n",
    "\n",
    "Responses must be based exclusively on information available before or during 2018.\n",
    "\n",
    "When an error in the question is identified (e.g., misspelling, wrong movie number) but points to a primary, well-known entity, provide the correct, assumed information directly.\n",
    "\n",
    "Your response must consist solely of the requested facts. Prohibit all greetings, introductions, explanations, notes, or any extraneous text.\n",
    "\n",
    "No use of abbreviations, acronyms, or initialisms. Provide full names and complete terms.\n",
    "\n",
    "No use of terms of puntuation like . , or ;\n",
    "\n",
    "If the answer requires multiple components (e.g., names, locations, dates), you must provide all essential components in their complete form.\n",
    "\n",
    "Context: Assume a general geographic or cultural context for interpretation.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2db13d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_rag(passages, question):\n",
    "    \"\"\"\n",
    "    Construye un prompt de RAG, inyectando pasajes de contexto antes de la pregunta.\n",
    "    \"\"\"\n",
    "    # Usamos el PROMPT_BASE como plantilla para las instrucciones\n",
    "    prompt = PROMPT_BASE.replace(\". Question: {question}:\", \"\") # Quitamos la parte de la pregunta final\n",
    "    \n",
    "    prompt += \"\\n\\nYou have to take the information of the answer from the following passages:\\n\\n\"\n",
    "\n",
    "    # Inyectar los pasajes de contexto\n",
    "    for i, p in enumerate(passages):\n",
    "        # Usamos .get(\"text\", ...) para obtener el texto del pasaje\n",
    "        text_content = p.get(\"text\", p.get(\"texto\", \"Pasaje sin contenido\")) # Si usa 'text' o 'texto'\n",
    "        \n",
    "        # Formatear el pasaje\n",
    "        prompt += f\"Passage {i + 1}:\\n{text_content}\\n---\\n\"\n",
    "\n",
    "    prompt += (\n",
    "                \"\\nNow answer the next question following the same style and using ONLY the provided information:\\n\"\n",
    "                f\"Question: {question}\"\n",
    "            )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Mantengo la funciÃ³n few-shot original solo por si la necesitas mÃ¡s tarde\n",
    "def build_prompt_fewshot(examples, question):\n",
    "    # Esto es idÃ©ntico a tu funciÃ³n original, solo le cambiÃ© el nombre\n",
    "    prompt = PROMPT_BASE.replace(\". Question: {question}:\", \"\")\n",
    "    prompt += \"    Here are examples of questions and their correct answers:\\n\\n\"\n",
    "    for ex in examples:\n",
    "        q = ex[\"question\"]\n",
    "        a = \", \".join(ex[\"answer\"]) if isinstance(ex[\"answer\"], list) else ex[\"answer\"]\n",
    "        prompt += f\"Q: {q}\\nA: {a}\\n\\n\"\n",
    "    prompt += (\n",
    "            \"\\nNow answer the next question following the same style:\\n\"\n",
    "            f\". Question: {question}: \"\n",
    "        )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b6d517",
   "metadata": {},
   "source": [
    "Funcion de ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5ba3676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_ollama(question, context_data = None):\n",
    "    \"\"\"\n",
    "    Consulta a Ollama y devuelve la respuesta completa o un mensaje de error.\n",
    "    Ahora usa context_data que puede ser Few-Shot examples o RAG passages.\n",
    "    \"\"\"\n",
    "    if VARIANTE == \"few-shot\" and context_data:\n",
    "        prompt = build_prompt_fewshot(context_data, question)\n",
    "    elif VARIANTE == \"rag\" and context_data:\n",
    "        # ðŸš¨ CAMBIO: Usamos la funciÃ³n RAG\n",
    "        prompt = build_prompt_rag(context_data, question)\n",
    "    else:\n",
    "        # Prompt base si no hay contexto (se usa el PROMPT_BASE, que es PROMPT_TEMPLATE1 en tu cÃ³digo)\n",
    "        prompt = PROMPT_BASE.format(question=question) # O usa PROMPT_TEMPLATE3 si quieres el pensamiento paso a paso\n",
    "        \n",
    "    print(prompt) # Descomenta para ver el prompt completo\n",
    "    full_response = \"\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(OLLAMA_URL, json={\n",
    "            \"model\": MODEL,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": True,\n",
    "            \"options\": { \"temperature\": 0.1 }\n",
    "        }, timeout=120)\n",
    "\n",
    "        response.raise_for_status()\n",
    "\n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                part = json.loads(line)\n",
    "                if \"response\" in part:\n",
    "                    full_response += part[\"response\"]\n",
    "                if part.get(\"done\"):\n",
    "                    break\n",
    "        \n",
    "        return full_response.strip()\n",
    "        \n",
    "    except requests.exceptions.ConnectionError:\n",
    "        return f\"[ERROR: ConexiÃ³n fallida con Ollama en {OLLAMA_URL}]\"\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        return f\"[ERROR: HTTP {e}. Â¿Modelo '{MODEL}' instalado?]\"\n",
    "    except Exception as e:\n",
    "        return f\"[ERROR: Inesperado: {e}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e27c56f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Se cargaron 10 preguntas de prueba de 'train.jsonl'.\n",
      "âœ… Se cargaron 6928 pasajes (Contexto RAG) de \"corpus.json\".\n",
      "\n",
      "Primeras 3 preguntas de PRUEBA cargadas:\n",
      "  1: where did the vietnam war mainly take place...\n",
      "  2: when was krakauer considered a success as a writer...\n",
      "  3: who played mrs garrett's son on facts of life...\n",
      "\n",
      "VerificaciÃ³n de un ejemplo de CONTEXTO (Pasaje):\n",
      "  Pasaje ID: 064_001\n",
      "  Contenido (Inicio): 1815. The Battle of Waterloowas a very importantbattle fought in 1815. It was between the French arm...\n"
     ]
    }
   ],
   "source": [
    "ARCHIVO_CORPUS = \"corpus.json\" \n",
    "\n",
    "preguntas_cargadas = [] # Lista para las preguntas de PRUEBA\n",
    "preguntas_test = []     # Lista para los pasajes de CONTEXTO RAG\n",
    "\n",
    "try:\n",
    "    # 1. Cargar DATOS DE PRUEBA (ARCHIVO_JSONL: LÃ­nea por lÃ­nea)\n",
    "    with open(ARCHIVO_JSONL, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            data = json.loads(line)\n",
    "            preguntas_cargadas.append({\n",
    "                \"nÃºmero_pregunta\": i + 1,\n",
    "                \"pregunta\": data.get(\"question\", \"\")\n",
    "            })\n",
    "    \n",
    "    print(f\"âœ… Se cargaron {len(preguntas_cargadas)} preguntas de prueba de '{ARCHIVO_JSONL}'.\")\n",
    "\n",
    "    # 2. Cargar DATOS DE CONTEXTO RAG (ARCHIVO_CORPUS: JSON COMPLETO)\n",
    "    # ðŸš¨ CORRECCIÃ“N CLAVE: Usamos json.load(f) para leer todo el array/objeto JSON.\n",
    "    with open(ARCHIVO_CORPUS, \"r\", encoding=\"utf-8\") as f:\n",
    "        preguntas_test = json.load(f)\n",
    "    \n",
    "    # Verificamos que se haya cargado una lista de pasajes\n",
    "    if isinstance(preguntas_test, list):\n",
    "        print(f\"âœ… Se cargaron {len(preguntas_test)} pasajes (Contexto RAG) de \\\"{ARCHIVO_CORPUS}\\\".\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ ADVERTENCIA: El archivo '{ARCHIVO_CORPUS}' fue cargado, pero no es una lista ({type(preguntas_test)}).\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"âŒ ERROR: El archivo '{e.filename}' no se encontrÃ³. Â¡Verifica la ruta!\")\n",
    "    preguntas_cargadas = None\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"âŒ ERROR: Fallo al decodificar JSON en '{ARCHIVO_CORPUS}'. Revisa que el archivo sea un JSON vÃ¡lido. Detalle: {e}\")\n",
    "    preguntas_cargadas = None\n",
    "\n",
    "\n",
    "# --- VerificaciÃ³n de Carga RAG ---\n",
    "\n",
    "if preguntas_cargadas and preguntas_test and isinstance(preguntas_test, list) and len(preguntas_test) > 0:\n",
    "    print(\"\\nPrimeras 3 preguntas de PRUEBA cargadas:\")\n",
    "    for item in preguntas_cargadas[:3]:\n",
    "        print(f\"  {item['nÃºmero_pregunta']}: {item['pregunta'][:50]}...\")\n",
    "    \n",
    "    # ðŸš¨ VERIFICACIÃ“N RAG: Muestra el contenido del pasaje (clave 'text' o 'texto')\n",
    "    ejemplo_completo = preguntas_test[0]\n",
    "    print(\"\\nVerificaciÃ³n de un ejemplo de CONTEXTO (Pasaje):\")\n",
    "    \n",
    "    # Asumo la clave 'text' (preferido) o 'texto'\n",
    "    texto_pasaje = ejemplo_completo.get('text', ejemplo_completo.get('texto', 'N/A'))\n",
    "    \n",
    "    print(f\"  Pasaje ID: {ejemplo_completo.get('id', 'N/A')}\")\n",
    "    print(f\"  Contenido (Inicio): {texto_pasaje[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e814357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando consulta al LLM con la variante RAG...\n",
      "entramos y codificamos embeddings del corpus de pasajes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando Preguntas:   0%|                              | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an AI model highly specialized in factual information retrieval. You are operating within the temporal context of 2018. All answers provided must strictly reflect the knowledge, events, and state of affairs valid up to the end of that year.\n",
      "\n",
      "Your sole mission is to address the user's question with the absolute minimum number of words possible, delivering only the essential and requested information. If ambiguity arises, assume the user's intended question and prioritize the most probable correct answer without acknowledging the error.\n",
      "\n",
      "The strict instructions that you may follow are the next ones:\n",
      "\n",
      "Responses must be based exclusively on information available before or during 2018.\n",
      "\n",
      "When an error in the question is identified (e.g., misspelling, wrong movie number) but points to a primary, well-known entity, provide the correct, assumed information directly.\n",
      "\n",
      "Your response must consist solely of the requested facts. Prohibit all greetings, introductions, explanations, notes, or any extraneous text.\n",
      "\n",
      "No use of abbreviations, acronyms, or initialisms. Provide full names and complete terms.\n",
      "\n",
      "No use of terms of puntuation like . , or ;\n",
      "\n",
      "If the answer requires multiple components (e.g., names, locations, dates), you must provide all essential components in their complete form.\n",
      "\n",
      "Context: Assume a general geographic or cultural context for interpretation.\n",
      "\n",
      "You have to take the information of the answer from the following passages:\n",
      "\n",
      "Passage 1:\n",
      "North Vietnamand South Vietnam. The vietnam war mainly took place between North Vietnamand South Vietnam. The conflict between communist and capitalist countries was part of theCold War. The conflict between communist and capitalist countries was part of theCold War.\n",
      "---\n",
      "Passage 2:\n",
      "the Vietnam War mainly took place in Vietnam. the Vietnam War mainly took place in Vietnam. the Vietnam War mainly took place in Vietnam. the Vietnam War mainly took place in Vietnam. the Vietnam War mainly took place in Vietnam.\n",
      "---\n",
      "Passage 3:\n",
      "vietnam war mainly took place in vietnam. the vietnam war mainly took place in vietnam. the vietnam war mainly took place in vietnam. the vietnam war mainly took place in vietnam.\n",
      "---\n",
      "\n",
      "Now answer the next question following the same style and using ONLY the provided information:\n",
      "Question: where did the vietnam war mainly take place\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando Preguntas:  10%|â–ˆâ–ˆâ–                   | 1/10 [01:06<09:55, 66.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… P1 guardada correctamente.\n",
      "You are an AI model highly specialized in factual information retrieval. You are operating within the temporal context of 2018. All answers provided must strictly reflect the knowledge, events, and state of affairs valid up to the end of that year.\n",
      "\n",
      "Your sole mission is to address the user's question with the absolute minimum number of words possible, delivering only the essential and requested information. If ambiguity arises, assume the user's intended question and prioritize the most probable correct answer without acknowledging the error.\n",
      "\n",
      "The strict instructions that you may follow are the next ones:\n",
      "\n",
      "Responses must be based exclusively on information available before or during 2018.\n",
      "\n",
      "When an error in the question is identified (e.g., misspelling, wrong movie number) but points to a primary, well-known entity, provide the correct, assumed information directly.\n",
      "\n",
      "Your response must consist solely of the requested facts. Prohibit all greetings, introductions, explanations, notes, or any extraneous text.\n",
      "\n",
      "No use of abbreviations, acronyms, or initialisms. Provide full names and complete terms.\n",
      "\n",
      "No use of terms of puntuation like . , or ;\n",
      "\n",
      "If the answer requires multiple components (e.g., names, locations, dates), you must provide all essential components in their complete form.\n",
      "\n",
      "Context: Assume a general geographic or cultural context for interpretation.\n",
      "\n",
      "You have to take the information of the answer from the following passages:\n",
      "\n",
      "Passage 1:\n",
      "Between 1984 and 2005, Krakauer was considered a success as a writer. The following is a list of Krakauer's novels: Between 1984 and 2005. The following is a list of Krakauer's novels: Between 1984 and 2005.\n",
      "---\n",
      "Passage 2:\n",
      "Jon Krakauer has won a slew of awards, including Time magazineâ€™s Book of the Year, an Academy Award for Literature, the Walter Sullivan Award for Literature, and the Walter Sullivan Award for Literature\n",
      "---\n",
      "Passage 3:\n",
      "1977, Krakauer spent three weeks alone in the wilderness of theStikine Icecapregion ofAlaskaand climbed a new route. He is considered a success as a writer.. in 1977.. in 1977.\n",
      "---\n",
      "\n",
      "Now answer the next question following the same style and using ONLY the provided information:\n",
      "Question: when was krakauer considered a success as a writer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando Preguntas:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–                 | 2/10 [01:31<05:38, 42.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… P2 guardada correctamente.\n",
      "You are an AI model highly specialized in factual information retrieval. You are operating within the temporal context of 2018. All answers provided must strictly reflect the knowledge, events, and state of affairs valid up to the end of that year.\n",
      "\n",
      "Your sole mission is to address the user's question with the absolute minimum number of words possible, delivering only the essential and requested information. If ambiguity arises, assume the user's intended question and prioritize the most probable correct answer without acknowledging the error.\n",
      "\n",
      "The strict instructions that you may follow are the next ones:\n",
      "\n",
      "Responses must be based exclusively on information available before or during 2018.\n",
      "\n",
      "When an error in the question is identified (e.g., misspelling, wrong movie number) but points to a primary, well-known entity, provide the correct, assumed information directly.\n",
      "\n",
      "Your response must consist solely of the requested facts. Prohibit all greetings, introductions, explanations, notes, or any extraneous text.\n",
      "\n",
      "No use of abbreviations, acronyms, or initialisms. Provide full names and complete terms.\n",
      "\n",
      "No use of terms of puntuation like . , or ;\n",
      "\n",
      "If the answer requires multiple components (e.g., names, locations, dates), you must provide all essential components in their complete form.\n",
      "\n",
      "Context: Assume a general geographic or cultural context for interpretation.\n",
      "\n",
      "You have to take the information of the answer from the following passages:\n",
      "\n",
      "Passage 1:\n",
      "Kim Fields played Mrs. Garrett's son on The Facts of Life. Fields portrayed a teenager in the show but was only nine when the first season aired. Kim Fields played Mrs. Garrett's son on The Facts of Life.\n",
      "---\n",
      "Passage 2:\n",
      "Claire Leachmanas played Mrs. Garrett's son on Facts of Life. Which actress played Mrs. Garrett's son on Facts of Life?'s first season.'s first season.'s last season.'s last season.'s last season.\n",
      "---\n",
      "Passage 3:\n",
      "Kristina Cohnas Nancy McKeonas Jo Polniaczek.she played Mrs. Garrett's son on the facts of life.she was a house mother on the facts of life.\n",
      "---\n",
      "\n",
      "Now answer the next question following the same style and using ONLY the provided information:\n",
      "Question: who played mrs garrett's son on facts of life\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando Preguntas:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ               | 3/10 [01:56<03:59, 34.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… P3 guardada correctamente.\n",
      "You are an AI model highly specialized in factual information retrieval. You are operating within the temporal context of 2018. All answers provided must strictly reflect the knowledge, events, and state of affairs valid up to the end of that year.\n",
      "\n",
      "Your sole mission is to address the user's question with the absolute minimum number of words possible, delivering only the essential and requested information. If ambiguity arises, assume the user's intended question and prioritize the most probable correct answer without acknowledging the error.\n",
      "\n",
      "The strict instructions that you may follow are the next ones:\n",
      "\n",
      "Responses must be based exclusively on information available before or during 2018.\n",
      "\n",
      "When an error in the question is identified (e.g., misspelling, wrong movie number) but points to a primary, well-known entity, provide the correct, assumed information directly.\n",
      "\n",
      "Your response must consist solely of the requested facts. Prohibit all greetings, introductions, explanations, notes, or any extraneous text.\n",
      "\n",
      "No use of abbreviations, acronyms, or initialisms. Provide full names and complete terms.\n",
      "\n",
      "No use of terms of puntuation like . , or ;\n",
      "\n",
      "If the answer requires multiple components (e.g., names, locations, dates), you must provide all essential components in their complete form.\n",
      "\n",
      "Context: Assume a general geographic or cultural context for interpretation.\n",
      "\n",
      "You have to take the information of the answer from the following passages:\n",
      "\n",
      "Passage 1:\n",
      "2018 season. The men's ice hockey winter olympics 2018 are scheduled for February 21 and February 20. The U.S. Women's Team will face Canada for GOLD Wednesday at 11:10pm ET on @NBCSN.\n",
      "---\n",
      "Passage 2:\n",
      "2018 Winter olympics â€“ Men's ice hockey winter olympics was held on 12â€“27 February 2018. The 2018 Winter olympics were held in Winter. What country did the men's ice hockey winter olympics take place in?\n",
      "---\n",
      "Passage 3:\n",
      "The men's ice hockey winter olympics 2018 were held on January 1 , 2018. The date was January 1 , 2018. The date was February 14-25 , 2018. The date was January 1 , 2018. The date was February 14-25 , 2018.\n",
      "---\n",
      "\n",
      "Now answer the next question following the same style and using ONLY the provided information:\n",
      "Question: when is the men's ice hockey winter olympics 2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando Preguntas:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 4/10 [02:27<03:17, 32.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… P4 guardada correctamente.\n",
      "You are an AI model highly specialized in factual information retrieval. You are operating within the temporal context of 2018. All answers provided must strictly reflect the knowledge, events, and state of affairs valid up to the end of that year.\n",
      "\n",
      "Your sole mission is to address the user's question with the absolute minimum number of words possible, delivering only the essential and requested information. If ambiguity arises, assume the user's intended question and prioritize the most probable correct answer without acknowledging the error.\n",
      "\n",
      "The strict instructions that you may follow are the next ones:\n",
      "\n",
      "Responses must be based exclusively on information available before or during 2018.\n",
      "\n",
      "When an error in the question is identified (e.g., misspelling, wrong movie number) but points to a primary, well-known entity, provide the correct, assumed information directly.\n",
      "\n",
      "Your response must consist solely of the requested facts. Prohibit all greetings, introductions, explanations, notes, or any extraneous text.\n",
      "\n",
      "No use of abbreviations, acronyms, or initialisms. Provide full names and complete terms.\n",
      "\n",
      "No use of terms of puntuation like . , or ;\n",
      "\n",
      "If the answer requires multiple components (e.g., names, locations, dates), you must provide all essential components in their complete form.\n",
      "\n",
      "Context: Assume a general geographic or cultural context for interpretation.\n",
      "\n",
      "You have to take the information of the answer from the following passages:\n",
      "\n",
      "Passage 1:\n",
      "bmw of north america v. gore case excessive punitive damages violate the rights of bmw of north america v. gore. bmw of north america v. gore case. (c) bmw of north america v. gore case\n",
      "---\n",
      "Passage 2:\n",
      "the principle of fairness in the workplace.. bmw of north america v. gore 's case stated that the bmw of north america v. gore case did not state that excessive punitive damages violated the principle of fairness in the workplace.\n",
      "---\n",
      "Passage 3:\n",
      "The bmw of north america v. gore case alleged excessive punitive damages violated the copyright of the BMW of North America v. Gore case. This is the legal text of the bmw of north america v. gore case.\n",
      "---\n",
      "\n",
      "Now answer the next question following the same style and using ONLY the provided information:\n",
      "Question: according to the bmw of north america v. gore case excessive punitive damages violate what\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando Preguntas:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 5/10 [02:56<02:38, 31.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… P5 guardada correctamente.\n",
      "You are an AI model highly specialized in factual information retrieval. You are operating within the temporal context of 2018. All answers provided must strictly reflect the knowledge, events, and state of affairs valid up to the end of that year.\n",
      "\n",
      "Your sole mission is to address the user's question with the absolute minimum number of words possible, delivering only the essential and requested information. If ambiguity arises, assume the user's intended question and prioritize the most probable correct answer without acknowledging the error.\n",
      "\n",
      "The strict instructions that you may follow are the next ones:\n",
      "\n",
      "Responses must be based exclusively on information available before or during 2018.\n",
      "\n",
      "When an error in the question is identified (e.g., misspelling, wrong movie number) but points to a primary, well-known entity, provide the correct, assumed information directly.\n",
      "\n",
      "Your response must consist solely of the requested facts. Prohibit all greetings, introductions, explanations, notes, or any extraneous text.\n",
      "\n",
      "No use of abbreviations, acronyms, or initialisms. Provide full names and complete terms.\n",
      "\n",
      "No use of terms of puntuation like . , or ;\n",
      "\n",
      "If the answer requires multiple components (e.g., names, locations, dates), you must provide all essential components in their complete form.\n",
      "\n",
      "Context: Assume a general geographic or cultural context for interpretation.\n",
      "\n",
      "You have to take the information of the answer from the following passages:\n",
      "\n",
      "Passage 1:\n",
      "the film society of the spectacle is an American film directed by Guy Debord. The film society of the spectacle is a American film directed by Guy Debord. The film society of the spectacle is a film directed by Guy Debord.\n",
      "---\n",
      "Passage 2:\n",
      "GÃ¶ran Hugo Olsson and Roxy Farhat. The Society of the Spectacle is a film by GÃ¶ran Hugo Olsson and Roxy Farhat. The Society of the Spectacle is a film by GÃ¶ran Hugo Olsson and Roxy Farhat.\n",
      "---\n",
      "Passage 3:\n",
      "Guy Debord..Six years after the publication of his Situationist classic The Society of the Spectacle, Guy Debord released this semi-experimental, essay-film adaptation. Using the classic Situationist technique of â€œdÃ©tournementâ€ (think pre-digital remixing), Debord overlays a dizzying array of still and film images with text from the book.\n",
      "---\n",
      "\n",
      "Now answer the next question following the same style and using ONLY the provided information:\n",
      "Question: who made the film society of the spectacle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando Preguntas:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 6/10 [03:25<02:02, 30.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… P6 guardada correctamente.\n",
      "You are an AI model highly specialized in factual information retrieval. You are operating within the temporal context of 2018. All answers provided must strictly reflect the knowledge, events, and state of affairs valid up to the end of that year.\n",
      "\n",
      "Your sole mission is to address the user's question with the absolute minimum number of words possible, delivering only the essential and requested information. If ambiguity arises, assume the user's intended question and prioritize the most probable correct answer without acknowledging the error.\n",
      "\n",
      "The strict instructions that you may follow are the next ones:\n",
      "\n",
      "Responses must be based exclusively on information available before or during 2018.\n",
      "\n",
      "When an error in the question is identified (e.g., misspelling, wrong movie number) but points to a primary, well-known entity, provide the correct, assumed information directly.\n",
      "\n",
      "Your response must consist solely of the requested facts. Prohibit all greetings, introductions, explanations, notes, or any extraneous text.\n",
      "\n",
      "No use of abbreviations, acronyms, or initialisms. Provide full names and complete terms.\n",
      "\n",
      "No use of terms of puntuation like . , or ;\n",
      "\n",
      "If the answer requires multiple components (e.g., names, locations, dates), you must provide all essential components in their complete form.\n",
      "\n",
      "Context: Assume a general geographic or cultural context for interpretation.\n",
      "\n",
      "You have to take the information of the answer from the following passages:\n",
      "\n",
      "Passage 1:\n",
      "Queen Elizabeth II became the queen of the UK in July 2016. She was the first queen of the UK to be crowned in the United Kingdom. Queen Elizabeth II became the first queen of the UK in the United Kingdom in 2016. She was the first queen of the United Kingdom to be crowned in the United Kingdom.\n",
      "---\n",
      "Passage 2:\n",
      "The Queen became the Queen of the United Kingdom in 1819. The Queen of England was crowned in 2025. The Queen of England was crowned in 1852. The Queen of England was born in 1839. The Queen of England was born in 1850.\n",
      "---\n",
      "Passage 3:\n",
      "Queen Elizabeth became Queen at age 25 upon the death on 6 February 1952 of her father, King George VI. Her uncle abdicated the throne on 11 December 1936, leaving the monarchy to her father King George VI. At 11, the eldest daughter of George VI became the heir to the British throne.\n",
      "---\n",
      "\n",
      "Now answer the next question following the same style and using ONLY the provided information:\n",
      "Question: when did the queen became the queen of the uk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando Preguntas:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 7/10 [03:57<01:33, 31.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… P7 guardada correctamente.\n",
      "You are an AI model highly specialized in factual information retrieval. You are operating within the temporal context of 2018. All answers provided must strictly reflect the knowledge, events, and state of affairs valid up to the end of that year.\n",
      "\n",
      "Your sole mission is to address the user's question with the absolute minimum number of words possible, delivering only the essential and requested information. If ambiguity arises, assume the user's intended question and prioritize the most probable correct answer without acknowledging the error.\n",
      "\n",
      "The strict instructions that you may follow are the next ones:\n",
      "\n",
      "Responses must be based exclusively on information available before or during 2018.\n",
      "\n",
      "When an error in the question is identified (e.g., misspelling, wrong movie number) but points to a primary, well-known entity, provide the correct, assumed information directly.\n",
      "\n",
      "Your response must consist solely of the requested facts. Prohibit all greetings, introductions, explanations, notes, or any extraneous text.\n",
      "\n",
      "No use of abbreviations, acronyms, or initialisms. Provide full names and complete terms.\n",
      "\n",
      "No use of terms of puntuation like . , or ;\n",
      "\n",
      "If the answer requires multiple components (e.g., names, locations, dates), you must provide all essential components in their complete form.\n",
      "\n",
      "Context: Assume a general geographic or cultural context for interpretation.\n",
      "\n",
      "You have to take the information of the answer from the following passages:\n",
      "\n",
      "Passage 1:\n",
      "The heartbeats of hummingbirds range between 50 and 200 times per minute. Their wings are between 50 and 200 times per second. The heartbeats of hummingbirds range between 50 and 200 times per minute. Their wings are between 50 and 200 times per second.\n",
      "---\n",
      "Passage 2:\n",
      "A hummingbird's heartbeat is equal to 2 beats per minute. This means that it's a bird that's heartbeat is about 10 beats per minute. The average heartbeat is around 5 beats per minute.\n",
      "---\n",
      "Passage 3:\n",
      "A hummingbird heartbeats every minute for about 7.5 seconds. The heartbeat rate is 1 / 5 seconds per minute. The heart rate is 1 / 5 seconds per minute. The heartbeat rate is 1 / 5 seconds per minute.\n",
      "---\n",
      "\n",
      "Now answer the next question following the same style and using ONLY the provided information:\n",
      "Question: how many times does a hummingbird heartbeat per minute\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando Preguntas:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 8/10 [04:24<00:59, 29.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… P8 guardada correctamente.\n",
      "You are an AI model highly specialized in factual information retrieval. You are operating within the temporal context of 2018. All answers provided must strictly reflect the knowledge, events, and state of affairs valid up to the end of that year.\n",
      "\n",
      "Your sole mission is to address the user's question with the absolute minimum number of words possible, delivering only the essential and requested information. If ambiguity arises, assume the user's intended question and prioritize the most probable correct answer without acknowledging the error.\n",
      "\n",
      "The strict instructions that you may follow are the next ones:\n",
      "\n",
      "Responses must be based exclusively on information available before or during 2018.\n",
      "\n",
      "When an error in the question is identified (e.g., misspelling, wrong movie number) but points to a primary, well-known entity, provide the correct, assumed information directly.\n",
      "\n",
      "Your response must consist solely of the requested facts. Prohibit all greetings, introductions, explanations, notes, or any extraneous text.\n",
      "\n",
      "No use of abbreviations, acronyms, or initialisms. Provide full names and complete terms.\n",
      "\n",
      "No use of terms of puntuation like . , or ;\n",
      "\n",
      "If the answer requires multiple components (e.g., names, locations, dates), you must provide all essential components in their complete form.\n",
      "\n",
      "Context: Assume a general geographic or cultural context for interpretation.\n",
      "\n",
      "You have to take the information of the answer from the following passages:\n",
      "\n",
      "Passage 1:\n",
      "The Harry Potter series was released between 1898 and 2000. The last book in the series was published in 1933. The first three books were published in 1954. The first four books were published in 1953. The last three books were published in 1954.\n",
      "---\n",
      "Passage 2:\n",
      "The first book,Harry Potter and the Philosopher's Stone, was published in 1997 by Bloomsburyin London. The final book,Harry Potter and the Deathly Hallows, sold more than 12 million copies in theU.S. Beginning on6 October2015, the entire series was presented in a fully-illustrated format, with over 100 illustrations per title, by Jim Kay.\n",
      "---\n",
      "Passage 3:\n",
      "The first book,Harry Potter and the Philosopher's Stone, was published in 1997 by Bloomsburyin London. The final book,Harry Potter and the Deathly Hallows, sold more than 12 million copies in theU.S. Beginning on6 October2015, the entire series was presented in a fully-illustrated format, with over 100 illustrations per title, by Jim Kay.[1]\n",
      "---\n",
      "\n",
      "Now answer the next question following the same style and using ONLY the provided information:\n",
      "Question: when were each of the harry potter books released\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando Preguntas:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 9/10 [05:12<00:35, 35.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… P9 guardada correctamente.\n",
      "You are an AI model highly specialized in factual information retrieval. You are operating within the temporal context of 2018. All answers provided must strictly reflect the knowledge, events, and state of affairs valid up to the end of that year.\n",
      "\n",
      "Your sole mission is to address the user's question with the absolute minimum number of words possible, delivering only the essential and requested information. If ambiguity arises, assume the user's intended question and prioritize the most probable correct answer without acknowledging the error.\n",
      "\n",
      "The strict instructions that you may follow are the next ones:\n",
      "\n",
      "Responses must be based exclusively on information available before or during 2018.\n",
      "\n",
      "When an error in the question is identified (e.g., misspelling, wrong movie number) but points to a primary, well-known entity, provide the correct, assumed information directly.\n",
      "\n",
      "Your response must consist solely of the requested facts. Prohibit all greetings, introductions, explanations, notes, or any extraneous text.\n",
      "\n",
      "No use of abbreviations, acronyms, or initialisms. Provide full names and complete terms.\n",
      "\n",
      "No use of terms of puntuation like . , or ;\n",
      "\n",
      "If the answer requires multiple components (e.g., names, locations, dates), you must provide all essential components in their complete form.\n",
      "\n",
      "Context: Assume a general geographic or cultural context for interpretation.\n",
      "\n",
      "You have to take the information of the answer from the following passages:\n",
      "\n",
      "Passage 1:\n",
      "The first papa john's opened in 1909. - March 9, 1905. - March 9, 1907. - March 9, 1908. - March 9, 1905. - March 9, 1909. - March 9, 1905.\n",
      "---\n",
      "Passage 2:\n",
      "The first papa john's opened in 1879. It was named after the famous pizza chef, Papa John's, who also owned the 'Pizza Sosu'. The restaurant also opened its own restaurant in 1892.\n",
      "---\n",
      "Passage 3:\n",
      "1872. The first Papa John's opened in the United States. The first Papa John's opened in 1921. The first Papa John's opened in 1924. The first Papa John's opened in 1933. The first Papa John's opened in 1936.\n",
      "---\n",
      "\n",
      "Now answer the next question following the same style and using ONLY the provided information:\n",
      "Question: when did the first papa john's open\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando Preguntas: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [05:39<00:00, 33.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… P10 guardada correctamente.\n",
      "\n",
      "âœ… EvaluaciÃ³n finalizada. Revisa el archivo 'rag.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Encabezados (aunque no los vamos a escribir, los necesitamos para el DataFrame)\n",
    "HEADERS = [\"nÃºmero_pregunta\", \"pregunta\", \"salida_llm\", \"nombre_variante\"]\n",
    "\n",
    "if preguntas_cargadas is None:\n",
    "    print(\"No se puede ejecutar el bucle porque las preguntas no se cargaron correctamente.\")\n",
    "else:\n",
    "    print(f\"\\nIniciando consulta al LLM con la variante RAG...\")\n",
    "    \n",
    "    es_primera_escritura = True\n",
    "\n",
    "    # model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "    train_data = preguntas_test # El corpus de pasajes\n",
    "    train_embeddings = None\n",
    "    \n",
    "    # --- 1. PREPARACIÃ“N DE EMBEDDINGS RAG ---\n",
    "    if VARIANTE == \"rag\":\n",
    "        print(\"entramos y codificamos embeddings del corpus de pasajes\")\n",
    "        # ðŸš¨ CORRECCIÃ“N CLAVE: Codificamos el 'text' (o 'texto') de los pasajes\n",
    "        # Â¡IMPORTANTE! AsegÃºrate de que la clave 'text' existe en tu corpus.\n",
    "        text_key = \"texto\" \n",
    "        if train_data and isinstance(train_data, list) and text_key in train_data[0]:\n",
    "            train_texts = [ex.get(text_key, \"\") for ex in train_data]\n",
    "            train_embeddings = model.encode(train_texts, convert_to_tensor=True)\n",
    "        else:\n",
    "            print(f\"âŒ ERROR CRÃTICO: La clave '{text_key}' no se encontrÃ³ en el corpus para RAG.\")\n",
    "            train_embeddings = None\n",
    "    \n",
    "    \n",
    "    # --- 2. BUCLE DE PROCESAMIENTO ---\n",
    "    for item in tqdm(preguntas_cargadas, desc=\"Procesando Preguntas\"):\n",
    "        \n",
    "        numero_pregunta = item[\"nÃºmero_pregunta\"]\n",
    "        question = item[\"pregunta\"]\n",
    "        \n",
    "        # AlmacenarÃ¡ los pasajes recuperados\n",
    "        context_data = None \n",
    "\n",
    "        if VARIANTE == \"rag\" and train_embeddings is not None:\n",
    "            # âž¡ï¸ LÃ³gica de RecuperaciÃ³n de Pasajes (RAG)\n",
    "            test_embedding = model.encode(question, convert_to_tensor=True)\n",
    "            cos_scores = util.pytorch_cos_sim(test_embedding, train_embeddings)[0]\n",
    "            top_results = cos_scores.topk(3)\n",
    "            \n",
    "            # Los 3 pasajes completos recuperados (diccionarios con 'text', 'id', etc.)\n",
    "            context_data = [train_data[idx] for idx in top_results[1]]\n",
    "        \n",
    "        \n",
    "        # Se pasa la pregunta y los pasajes recuperados a ask_ollama\n",
    "        salida_llm = ask_ollama(question, context_data=context_data)\n",
    "        \n",
    "        # ... (Resto de la lÃ³gica de guardado, sin cambios)\n",
    "        salida_llm = re.sub(r'\\s+', ' ', salida_llm).strip()\n",
    "        \n",
    "        df_fila = pd.DataFrame([{\n",
    "            \"nÃºmero_pregunta\": numero_pregunta,\n",
    "            \"pregunta\": question,\n",
    "            \"salida_llm\": salida_llm,\n",
    "            \"nombre_variante\": VARIANTE\n",
    "        }], columns=HEADERS)\n",
    "        \n",
    "        # Define el modo de escritura: 'w' (write/sobrescribir) para la primera, 'a' (append/aÃ±adir) despuÃ©s\n",
    "        modo_escritura = 'w' if es_primera_escritura else 'a'\n",
    "        \n",
    "        try:\n",
    "            # Crea el archivo con encabezados solo en la primera iteraciÃ³n si es necesario\n",
    "            if es_primera_escritura:\n",
    "                 pd.DataFrame(columns=HEADERS).to_csv(ARCHIVO_CSV_SALIDA, mode='w', header=True, index=False, encoding='utf-8')\n",
    "                 modo_escritura = 'a'\n",
    "            \n",
    "            df_fila.to_csv(\n",
    "                ARCHIVO_CSV_SALIDA,\n",
    "                mode=modo_escritura, # Usa el modo dinÃ¡mico\n",
    "                header=False,        \n",
    "                index=False,\n",
    "                encoding='utf-8',\n",
    "                lineterminator='\\n'  \n",
    "            )\n",
    "            print(f\"âœ… P{numero_pregunta} guardada correctamente.\")\n",
    "            es_primera_escritura = False # Desactiva la bandera despuÃ©s de la primera escritura\n",
    "        except Exception as e:\n",
    "            print(f\"\\nâŒ ERROR al guardar la pregunta {numero_pregunta} en CSV: {e}\")\n",
    "            \n",
    "        time.sleep(0.1)\n",
    "\n",
    "    print(f\"\\nâœ… EvaluaciÃ³n finalizada. Revisa el archivo '{ARCHIVO_CSV_SALIDA}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e1a5987-68ef-4169-83a3-d1b7c4682f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ’¡ NUEVA CELDA A: DefiniciÃ³n de la FunciÃ³n de F1 Score\n",
    "import collections\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Quita artÃ­culos, puntuaciÃ³n y normaliza el texto a minÃºsculas.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        # Remueve 'a', 'an', 'the' al inicio o entre palabras\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        # Normaliza espacios en blanco\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        # Remueve puntuaciÃ³n\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        # Convierte a minÃºsculas\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def calculate_f1_score(generated_answer, expected_answers_list):\n",
    "    \"\"\"\n",
    "    Calcula el F1 Score de la respuesta generada contra una lista de respuestas correctas.\n",
    "    Retorna el F1 score mÃ¡s alto encontrado.\n",
    "    \"\"\"\n",
    "    if not expected_answers_list or expected_answers_list == [\"N/A\"]:\n",
    "        return 0.0\n",
    "\n",
    "    gen_tokens = normalize_answer(generated_answer).split()\n",
    "    best_f1 = 0.0\n",
    "    \n",
    "    for expected in expected_answers_list:\n",
    "        exp_tokens = normalize_answer(expected).split()\n",
    "        \n",
    "        if not gen_tokens and not exp_tokens:\n",
    "            return 1.0\n",
    "\n",
    "        common = collections.Counter(gen_tokens) & collections.Counter(exp_tokens)\n",
    "        num_common = sum(common.values())\n",
    "\n",
    "        if num_common == 0:\n",
    "            f1 = 0.0\n",
    "        else:\n",
    "            precision = num_common / len(gen_tokens)\n",
    "            recall = num_common / len(exp_tokens)\n",
    "            f1 = (2 * precision * recall) / (precision + recall)\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "\n",
    "    return best_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8c8438f-bc2c-46f2-9093-d35a20f7bc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Se cargaron 10 respuestas esperadas de 'train.jsonl'.\n",
      "âœ… Se cargaron 10 resultados del LLM de 'rag.csv'.\n"
     ]
    }
   ],
   "source": [
    "# ðŸ’¡ NUEVA CELDA B: Carga de resultados y Ground Truth\n",
    "\n",
    "# Asumimos que estas variables globales ya existen desde la Celda 5:\n",
    "# ARCHIVO_JSONL = \"train.jsonl\"\n",
    "# ARCHIVO_CSV_SALIDA = \"rag.csv\" \n",
    "\n",
    "try:\n",
    "    # 1. Cargar el Ground Truth (respuestas esperadas) de train.jsonl\n",
    "    with open(ARCHIVO_JSONL, \"r\", encoding=\"utf-8\") as f:\n",
    "        # Usamos list comprehension para eficiencia\n",
    "        ground_truth_list = [json.loads(line) for line in f]\n",
    "\n",
    "    # Crear diccionario de bÃºsqueda: {pregunta normalizada: [respuestas esperadas]}\n",
    "    ground_truth_map = {\n",
    "        normalize_answer(item['question']): item.get('answer', [\"N/A\"]) \n",
    "        for item in ground_truth_list\n",
    "    }\n",
    "    \n",
    "    print(f\"âœ… Se cargaron {len(ground_truth_map)} respuestas esperadas de '{ARCHIVO_JSONL}'.\")\n",
    "\n",
    "    # 2. Cargar las respuestas generadas por el LLM\n",
    "    df_results = pd.read_csv(ARCHIVO_CSV_SALIDA, lineterminator='\\n', encoding='utf-8')\n",
    "    print(f\"âœ… Se cargaron {len(df_results)} resultados del LLM de '{ARCHIVO_CSV_SALIDA}'.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"âŒ ERROR: El archivo '{e.filename}' no se encontrÃ³. AsegÃºrate de que la Celda 16 se ejecutÃ³ completamente.\")\n",
    "    df_results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c6edc3c-024f-4604-b545-fab622000471",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculando F1 Scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 3067.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "        RESULTADOS FINALES DE EVALUACIÃ“N\n",
      "==================================================\n",
      "   Modelo Evaluado: llama3.1:8b (rag)\n",
      "   NÃºmero de Preguntas Evaluadas: 10\n",
      "--------------------------------------------------\n",
      "   F1 Score Promedio (Micro-Averaged): 0.2799\n",
      "==================================================\n",
      "âœ… Resultados con F1 score guardados en 'rag_con_f1_score.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ðŸ’¡ NUEVA CELDA C: CÃ¡lculo de F1 y Reporte\n",
    "\n",
    "if df_results is not None:\n",
    "    f1_scores = []\n",
    "    \n",
    "    # Iteramos sobre cada resultado generado por el LLM (rag.csv)\n",
    "    for index, row in tqdm(df_results.iterrows(), total=len(df_results), desc=\"Calculando F1 Scores\"):\n",
    "        \n",
    "        generated_answer = str(row['salida_llm']) # Respuesta generada por Ollama\n",
    "        original_question = str(row['pregunta'])\n",
    "\n",
    "        # Buscamos el Ground Truth usando la pregunta normalizada como clave\n",
    "        normalized_question = normalize_answer(original_question)\n",
    "        \n",
    "        # Obtenemos la lista de respuestas esperadas (Ground Truth)\n",
    "        expected_answers = ground_truth_map.get(normalized_question, [\"N/A\"])\n",
    "        \n",
    "        # Calculamos el F1 score para esa pregunta\n",
    "        f1 = calculate_f1_score(generated_answer, expected_answers)\n",
    "        f1_scores.append(f1)\n",
    "        \n",
    "    # Agregamos los F1 scores calculados al DataFrame original\n",
    "    df_results['f1_score_calculado'] = f1_scores\n",
    "    \n",
    "    # ---------------------------------------------\n",
    "    # IMPRIMIR RESULTADOS FINALES\n",
    "    # ---------------------------------------------\n",
    "    \n",
    "    # Calculamos el promedio global (Macro F1 sobre el conjunto)\n",
    "    avg_f1 = np.mean(f1_scores)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"        RESULTADOS FINALES DE EVALUACIÃ“N\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"   Modelo Evaluado: {MODEL} ({VARIANTE})\")\n",
    "    print(f\"   NÃºmero de Preguntas Evaluadas: {len(df_results)}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"   F1 Score Promedio (Micro-Averaged): {avg_f1:.4f}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Opcional: Si quieres guardar los resultados CON el F1 score calculado:\n",
    "    ARCHIVO_F1_SALIDA = \"rag_con_f1_score.csv\"\n",
    "    df_results.to_csv(ARCHIVO_F1_SALIDA, index=False, encoding='utf-8', lineterminator='\\n')\n",
    "    print(f\"âœ… Resultados con F1 score guardados en '{ARCHIVO_F1_SALIDA}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a71744-4052-4f84-8079-686c176d45d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
